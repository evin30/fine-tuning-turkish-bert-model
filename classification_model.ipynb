{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "classification_model.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPkuKaSurETdXezP9pJSJlt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ktoprakucar/fine-tuning-turkish-bert-model/blob/master/classification_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lgcx4jPKRd8V",
        "colab_type": "text"
      },
      "source": [
        "koda geçmeden önce, koddaki BERT ile ilgili olan kısımları https://mccormickml.com/2019/07/22/BERT-fine-tuning/ linkindeki kodlardan yararlanarak oluşturduğumu belirtmek isterim"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RrYO6HDoRNNP",
        "colab_type": "code",
        "outputId": "ff3e2007-927f-4ab7-8882-a1a5fc3a0dd1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        }
      },
      "source": [
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import torch\n",
        "import numpy as np\n",
        "import time\n",
        "import datetime\n",
        "import random\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "% matplotlib inline\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "!pip install transformers\n",
        "import transformers\n",
        "from transformers import BertTokenizer\n",
        "from torch.utils.data import TensorDataset, random_split\n",
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.preprocessing import LabelEncoder"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (2.8.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.41)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.12.39)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.38.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.21.0)\n",
            "Requirement already satisfied: tokenizers==0.5.2 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.5.2)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.85)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.1)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.5)\n",
            "Requirement already satisfied: botocore<1.16.0,>=1.15.39 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.15.39)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.3.3)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.4.5.1)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.39->boto3->transformers) (0.15.2)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.39->boto3->transformers) (2.8.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OCCSYp9sSlSD",
        "colab_type": "code",
        "outputId": "757f1ac9-1f02-48a8-f6f5-44bc5c353040",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# check GPU\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name == '/device:GPU:0':\n",
        "    device = torch.device(\"cuda\")\n",
        "    print('GPU:', torch.cuda.get_device_name(0))\n",
        "else:\n",
        "    raise SystemError('GPU device not found')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GPU: Tesla P100-PCIE-16GB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2tc-DVN1S448",
        "colab_type": "code",
        "outputId": "6fbe5eb6-7bd8-4e85-bf2f-d63b5476365f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# set environment as googledrive to folder \"resource\"\n",
        "data_path =  \"/resource/\"\n",
        "\n",
        "try:\n",
        "    drive.mount('/content/drive')\n",
        "    data_path = \"/content/drive/My Drive/resource/\"\n",
        "\n",
        "except:\n",
        "    print(\"You are not working in Colab at the moment :(\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Nt6KR1ZTMEh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.read_csv(data_path + 'turkish_text_data.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vaWQQe3yTYab",
        "colab_type": "code",
        "outputId": "686980fd-9443-42bc-a868-01992fe62518",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "df.info()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 4900 entries, 0 to 4899\n",
            "Data columns (total 2 columns):\n",
            " #   Column    Non-Null Count  Dtype \n",
            "---  ------    --------------  ----- \n",
            " 0   category  4900 non-null   object\n",
            " 1   text      4900 non-null   object\n",
            "dtypes: object(2)\n",
            "memory usage: 76.7+ KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NOTi-A1UTc24",
        "colab_type": "code",
        "outputId": "1f0702da-b0d4-449f-e98e-c79d118e505a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        }
      },
      "source": [
        "df.sample(10)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>category</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>4376</th>\n",
              "      <td>teknoloji</td>\n",
              "      <td>kırık iphone kabusu çabuk tükenen bataryaları...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49</th>\n",
              "      <td>siyaset</td>\n",
              "      <td>kamut denetçileri eski ak_partili çıktı yargı...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2993</th>\n",
              "      <td>saglik</td>\n",
              "      <td>iki uzvunu kaybetti bir dönem hülya_koçyiğit ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4190</th>\n",
              "      <td>spor</td>\n",
              "      <td> çankaya belediye hak ettiği bir galibiyet a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>141</th>\n",
              "      <td>siyaset</td>\n",
              "      <td>fidan yasası yüksek_mahkeme de mit_müsteşarı_...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4209</th>\n",
              "      <td>teknoloji</td>\n",
              "      <td>apple iphone isim hakkını kaybetti apple raki...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3562</th>\n",
              "      <td>spor</td>\n",
              "      <td>ukraynalı golcü türkiye yi istiyor sezon başı...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4705</th>\n",
              "      <td>teknoloji</td>\n",
              "      <td>gangnam_style youtube u mest etti güney_korel...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>740</th>\n",
              "      <td>dunya</td>\n",
              "      <td>marihuana obama dan daha çok oy aldı başkanlı...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1608</th>\n",
              "      <td>ekonomi</td>\n",
              "      <td>toyota türkiye den 12 bin aracını geri çağırı...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        category                                               text\n",
              "4376  teknoloji    kırık iphone kabusu çabuk tükenen bataryaları...\n",
              "49      siyaset    kamut denetçileri eski ak_partili çıktı yargı...\n",
              "2993     saglik    iki uzvunu kaybetti bir dönem hülya_koçyiğit ...\n",
              "4190       spor     çankaya belediye hak ettiği bir galibiyet a...\n",
              "141     siyaset    fidan yasası yüksek_mahkeme de mit_müsteşarı_...\n",
              "4209  teknoloji    apple iphone isim hakkını kaybetti apple raki...\n",
              "3562       spor    ukraynalı golcü türkiye yi istiyor sezon başı...\n",
              "4705  teknoloji    gangnam_style youtube u mest etti güney_korel...\n",
              "740       dunya    marihuana obama dan daha çok oy aldı başkanlı...\n",
              "1608    ekonomi    toyota türkiye den 12 bin aracını geri çağırı..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FEcaS2orThoy",
        "colab_type": "code",
        "outputId": "3ba801d7-6efc-43f5-85b2-b9d9a6e589e1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "df.groupby('category').size()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "category\n",
              "dunya         700\n",
              "ekonomi       700\n",
              "kultur        700\n",
              "saglik        700\n",
              "siyaset       700\n",
              "spor          700\n",
              "teknoloji     700\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KOo9dA56_7Oy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df['encoded_categories'] = LabelEncoder().fit_transform(df['category'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wtmbRhroAtCd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('dbmdz/bert-base-turkish-128k-uncased', do_lower_case=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6DzbC8f7AxAC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentences = df.text.values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pPgQuTUGA1ct",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "max_len = 250"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AsHlvfa1A9J_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "training = df.groupby('category').apply(lambda x : x.sample(frac = 0.8))\n",
        "test = pd.concat([df,training]).drop_duplicates(keep=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qYiFqjplA_e4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "4debbf93-5b8e-4e14-fc83-177c02cc9fd0"
      },
      "source": [
        "print(\"Training: \", len(training))\n",
        "print(\"Test: \", len(test))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training:  3920\n",
            "Test:  821\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6PuTo9fHBBFd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "training_texts = training.text.values\n",
        "training_labels = training.encoded_categories.values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AavMUW7oBESR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 530
        },
        "outputId": "3a8b64fc-08a7-494a-fce0-7b71b65dcdee"
      },
      "source": [
        "input_ids = []\n",
        "attention_masks = []\n",
        "\n",
        "for text in training_texts:\n",
        "    encoded_dict = tokenizer.encode_plus(\n",
        "                        text,                     \n",
        "                        add_special_tokens = True,\n",
        "                        max_length = max_len,      \n",
        "                        pad_to_max_length = True,\n",
        "                        return_attention_mask = True, \n",
        "                        return_tensors = 'pt',\n",
        "                   )\n",
        "    \n",
        "    input_ids.append(encoded_dict['input_ids'])\n",
        "    attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "input_ids = torch.cat(input_ids, dim=0)\n",
        "attention_masks = torch.cat(attention_masks, dim=0)\n",
        "labels = torch.tensor(training_labels)\n",
        "\n",
        "print('Original: ', training_texts[0])\n",
        "print('Token IDs:', input_ids[0])"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original:   şehitlere saygısızlık işinden etti abd  de şehitlikte çektirdiği uygunsuz fotoğrafını facebook  ta yayınlayan lindsey_stone için  lindsey  i kovun  başlıklı bir sayfa açıldı binlerce kişinin sayfayı beğenmesinin ardından stone yardım derneğindeki işinden kovuldu abd  nin massachusetts eyaletinin cape cod koyunda yaşayan ve life adlı yardım derneğinde çalışan lindsey_stone ülkenin en önemli şehitliklerinden biri olan arlington  da bir askerin mezarı başında çektirdiği fotoğrafını facebook sayfasında yayınladı internette en çok paylaşılanlar arasına giren uygunuz fotoğraf kimliği belirsiz bir kişi tarafından açılan  lindsey_stone  u kovun  sayfasının da kapak karesi oldu çalıştığı şirket binlerce kişi tarafından beğenilen sayfanın yayınlanmasının ardından stone  u kovduğunu duyurdu engelli yetişkinler yararına çalışan dernek görevlisi 30 yaşındaki stone medyada geniş yer bulan fotoğraflarını iş arkadaşı jamie schuh ile birlikte çekti daily_mail gazetesinde yer alan habere göre facebook  ta açılan  lindsey_stone  u kovun  sayfası yaklaşık 19 bin kişi tarafından beğenildi yardım derneği yetkilileri bu sayfanın yayınlanmasının ardından facebook  ta bir açıklama yayınlayarak stone ve schuh  un görevlerine son verildiğini belirtti yetkililer açıklamada şehit yakınlarına şu sözlerle seslendi  arlington şehitliği olayında ismi geçen iki çalışanımızın işine son verdiğimizi duyurmak isteriz şehit ailelerine yapılan saygısızlıktan dolayı derin üzüntü içerisindeyiz  bu açıklamanın ardından kendilerini eleştirenlere sosyal medya üzerinden yanıt veren stone ve schuh fotoğrafı eğlenmek için çektiklerini amaçlarının kimseye saygısızlık etmek olmadığını söyledi\n",
            "Token IDs: tensor([     2, 121173,   2406,  26771,  38374,   1981,   2517,   2869,   1961,\n",
            "        121173,  10919, 115973,  11581,  32014,  24764, 123264,  18847,   6170,\n",
            "          2058,  30156, 101592,     41,  28618,   8059, 101592,     51,  74187,\n",
            "          1009,   2489,   4481,   1947,   3913,   4378,   9910,   1022,   5922,\n",
            "         57643,  22973,  29266,   5374,   3092,  28618,   2604, 106947,  17520,\n",
            "          2125,  38374,   1981,  81850,   2869,   2276,  49912,  34971,  46174,\n",
            "         87969,  98704,  51496,   1946,  18080,   4245,   2604, 106947,  17520,\n",
            "         52565,   1928, 101592,     41,  28618,  87717,   8117,   2064,  41541,\n",
            "        121173,  57510,   1981,   2797,   2120,   1991,  92876,   1972,   1947,\n",
            "         17932,  25400,  17088, 115973,  11581,  32014, 123264,  18847,   6170,\n",
            "         11623,  14522,  13201,   2064,   6110,  34586,   2744,   1934,   8211,\n",
            "          5485,  86268,   1015,  18248,   2507,  16133,  11432,   1947,  29291,\n",
            "          2418,   4378,   1959, 101592,     41,  28618,     63,  74187,   1009,\n",
            "         39627,   1972,   7831,  35164,   2069,  52565,   2022,   2893,  21876,\n",
            "          5922,  29291,   2418,  29266,   2227,  20947,  82626,   3092,  28618,\n",
            "            63,  83286,  11497,  10233,   8062, 125304,   1944,  16692,  52565,\n",
            "          1928,   6228,  67998,   4671,   3052,  55011,   2125,  28618,  10258,\n",
            "         40565,   2101,   7766,  98464,   2035,   2529,  13615,   1022,  41372,\n",
            "         65044,   1035,   2037,   2664, 115973,  25691,     41,   7257,  16375,\n",
            "          2101,   2309,  14712,  22586,   6170,   2058,   4378,   1959, 101592,\n",
            "            41,  28618,     63,  74187,   1009,   9927, 101061,   2375,   2473,\n",
            "         29291,   2418,  29266,   2756,   2604, 106947,   9320,   1964,  20947,\n",
            "         82626,   3092,   6170,   2058,   1947,  20314,   2131,  47789,  28618,\n",
            "          1946,  65044,   1035,   2397,  67998,   2616,   2039,   5799,   7426,\n",
            "          4944,   9483,  20314,  32084, 121173,  19366,   2366,  69718,   1939,\n",
            "         30942,   1991,  92876, 121173,  16133,  32131,   6330,  24727,   2537,\n",
            "         52565,   2109,  70928, 127419,   2039,  59397,  32598,  30749,  14597,\n",
            "        121173,  21367,   2649,  26771,   2117,   3776,      3])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T4YUhZxyCwVF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_dataset = TensorDataset(input_ids, attention_masks, labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cTUsi4QGBxQr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 32\n",
        "\n",
        "train_dataloader = DataLoader(\n",
        "            train_dataset,  \n",
        "            sampler = RandomSampler(train_dataset), \n",
        "            batch_size = batch_size \n",
        "        )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dxa9wOcVCzq8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "number_of_categories = len(df['encoded_categories'].unique())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QfNDxpE3C2bP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c8f55cba-e93d-48bc-9c8c-c128809182d7"
      },
      "source": [
        "model = BertForSequenceClassification.from_pretrained(\n",
        "    \"dbmdz/bert-base-turkish-128k-uncased\",\n",
        "    num_labels = number_of_categories, \n",
        "    output_attentions = False,\n",
        "    output_hidden_states = False,\n",
        ")\n",
        "\n",
        "model.cuda()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(128000, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=7, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wPL4qSPkC8hx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "epochs = 4\n",
        "\n",
        "optimizer = AdamW(model.parameters(),\n",
        "                  lr = 5e-5,\n",
        "                  eps = 1e-8 \n",
        "                )\n",
        "\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                            num_warmup_steps = 0,\n",
        "                                            num_training_steps = total_steps)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mqFHIKBhE9wv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def format_time(elapsed):\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Or7cXZA9DPs4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "bb9ca885-cf12-4d29-a361-15048fee9c7f"
      },
      "source": [
        "# This training code is based on the `run_glue.py` script here:\n",
        "# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n",
        "\n",
        "seed_val = 1903\n",
        "\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "training_stats = []\n",
        "total_t0 = time.time()\n",
        "\n",
        "for epoch_i in range(0, epochs):\n",
        "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "    t0 = time.time()\n",
        "    total_train_loss = 0\n",
        "    model.train()\n",
        "\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "        if step % 10 == 0 and not step == 0:\n",
        "            elapsed = format_time(time.time() - t0)\n",
        "            print('Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "\n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "\n",
        "        model.zero_grad()        \n",
        "        loss, logits = model(b_input_ids, \n",
        "                             token_type_ids=None, \n",
        "                             attention_mask=b_input_mask, \n",
        "                             labels=b_labels)\n",
        "        total_train_loss += loss.item()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "    avg_train_loss = total_train_loss / len(train_dataloader)            \n",
        "    training_time = format_time(time.time() - t0)\n",
        "\n",
        "    print(\"Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "    print(\"Training epoch took: {:}\".format(training_time))\n",
        "\n",
        "    training_stats.append(\n",
        "        {\n",
        "            'epoch': epoch_i + 1,\n",
        "            'Training Loss': avg_train_loss,\n",
        "            'Training Time': training_time,\n",
        "        }\n",
        "    )\n",
        "\n",
        "print(\"Training completed in {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "======== Epoch 1 / 4 ========\n",
            "Batch    10  of    123.    Elapsed: 0:00:08.\n",
            "Batch    20  of    123.    Elapsed: 0:00:15.\n",
            "Batch    30  of    123.    Elapsed: 0:00:23.\n",
            "Batch    40  of    123.    Elapsed: 0:00:31.\n",
            "Batch    50  of    123.    Elapsed: 0:00:38.\n",
            "Batch    60  of    123.    Elapsed: 0:00:46.\n",
            "Batch    70  of    123.    Elapsed: 0:00:54.\n",
            "Batch    80  of    123.    Elapsed: 0:01:01.\n",
            "Batch    90  of    123.    Elapsed: 0:01:09.\n",
            "Batch   100  of    123.    Elapsed: 0:01:17.\n",
            "Batch   110  of    123.    Elapsed: 0:01:24.\n",
            "Batch   120  of    123.    Elapsed: 0:01:32.\n",
            "Average training loss: 0.53\n",
            "Training epoch took: 0:01:34\n",
            "======== Epoch 2 / 4 ========\n",
            "Batch    10  of    123.    Elapsed: 0:00:08.\n",
            "Batch    20  of    123.    Elapsed: 0:00:15.\n",
            "Batch    30  of    123.    Elapsed: 0:00:23.\n",
            "Batch    40  of    123.    Elapsed: 0:00:31.\n",
            "Batch    50  of    123.    Elapsed: 0:00:38.\n",
            "Batch    60  of    123.    Elapsed: 0:00:46.\n",
            "Batch    70  of    123.    Elapsed: 0:00:54.\n",
            "Batch    80  of    123.    Elapsed: 0:01:01.\n",
            "Batch    90  of    123.    Elapsed: 0:01:09.\n",
            "Batch   100  of    123.    Elapsed: 0:01:17.\n",
            "Batch   110  of    123.    Elapsed: 0:01:24.\n",
            "Batch   120  of    123.    Elapsed: 0:01:32.\n",
            "Average training loss: 0.15\n",
            "Training epoch took: 0:01:34\n",
            "======== Epoch 3 / 4 ========\n",
            "Batch    10  of    123.    Elapsed: 0:00:08.\n",
            "Batch    20  of    123.    Elapsed: 0:00:15.\n",
            "Batch    30  of    123.    Elapsed: 0:00:23.\n",
            "Batch    40  of    123.    Elapsed: 0:00:31.\n",
            "Batch    50  of    123.    Elapsed: 0:00:38.\n",
            "Batch    60  of    123.    Elapsed: 0:00:46.\n",
            "Batch    70  of    123.    Elapsed: 0:00:54.\n",
            "Batch    80  of    123.    Elapsed: 0:01:01.\n",
            "Batch    90  of    123.    Elapsed: 0:01:09.\n",
            "Batch   100  of    123.    Elapsed: 0:01:17.\n",
            "Batch   110  of    123.    Elapsed: 0:01:24.\n",
            "Batch   120  of    123.    Elapsed: 0:01:32.\n",
            "Average training loss: 0.06\n",
            "Training epoch took: 0:01:34\n",
            "======== Epoch 4 / 4 ========\n",
            "Batch    10  of    123.    Elapsed: 0:00:08.\n",
            "Batch    20  of    123.    Elapsed: 0:00:15.\n",
            "Batch    30  of    123.    Elapsed: 0:00:23.\n",
            "Batch    40  of    123.    Elapsed: 0:00:31.\n",
            "Batch    50  of    123.    Elapsed: 0:00:38.\n",
            "Batch    60  of    123.    Elapsed: 0:00:46.\n",
            "Batch    70  of    123.    Elapsed: 0:00:53.\n",
            "Batch    80  of    123.    Elapsed: 0:01:01.\n",
            "Batch    90  of    123.    Elapsed: 0:01:09.\n",
            "Batch   100  of    123.    Elapsed: 0:01:16.\n",
            "Batch   110  of    123.    Elapsed: 0:01:24.\n",
            "Batch   120  of    123.    Elapsed: 0:01:32.\n",
            "Average training loss: 0.02\n",
            "Training epoch took: 0:01:34\n",
            "Training completed in 0:06:16 (h:mm:ss)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QEX93h5OE-pG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "e790e666-dd17-484b-b2c1-04c4535ddb73"
      },
      "source": [
        "df_stats = pd.DataFrame(data=training_stats)\n",
        "plt.plot(df_stats['Training Loss'], label=\"Training\")\n",
        "plt.title(\"Training Loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.xticks([1, 2, 3, 4])\n",
        "plt.show()"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xa3aIdMyJwm-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_texts = test.text.values\n",
        "test_labels = test.encoded_categories.values\n",
        "\n",
        "input_ids = []\n",
        "attention_masks = []\n",
        "\n",
        "for text in test_texts:\n",
        "    encoded_dict = tokenizer.encode_plus(\n",
        "                        text,                     \n",
        "                        add_special_tokens = True, \n",
        "                        max_length = max_len,          \n",
        "                        pad_to_max_length = True,\n",
        "                        return_attention_mask = True,  \n",
        "                        return_tensors = 'pt',   \n",
        "                   )\n",
        "    \n",
        "    input_ids.append(encoded_dict['input_ids'])\n",
        "    attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "input_ids = torch.cat(input_ids, dim=0)\n",
        "attention_masks = torch.cat(attention_masks, dim=0)\n",
        "labels = torch.tensor(test_labels)\n",
        "\n",
        "batch_size = 32  \n",
        "\n",
        "prediction_data = TensorDataset(input_ids, attention_masks, labels)\n",
        "prediction_sampler = SequentialSampler(prediction_data)\n",
        "prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8RNfzOIZKH82",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "8c84ed0d-9c2e-4586-d0d6-0c34bf6640df"
      },
      "source": [
        "print('Prediction started on test data')\n",
        "model.eval()\n",
        "predictions , true_labels = [], []\n",
        "\n",
        "for batch in prediction_dataloader:\n",
        "  batch = tuple(t.to(device) for t in batch)\n",
        "  b_input_ids, b_input_mask, b_labels = batch\n",
        "\n",
        "  with torch.no_grad():\n",
        "      outputs = model(b_input_ids, token_type_ids=None, \n",
        "                      attention_mask=b_input_mask)\n",
        "\n",
        "  logits = outputs[0]\n",
        "  logits = logits.detach().cpu().numpy()\n",
        "  label_ids = b_labels.to('cpu').numpy()\n",
        "  \n",
        "  predictions.append(logits)\n",
        "  true_labels.append(label_ids)\n",
        "\n",
        "print('Prediction completed')"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Prediction started on test data\n",
            "Prediction completed\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YNJdxER7KeBH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "prediction_set = []\n",
        "\n",
        "for i in range(len(true_labels)):\n",
        "  pred_labels_i = np.argmax(predictions[i], axis=1).flatten()\n",
        "  prediction_set.append(pred_labels_i)\n",
        "\n",
        "prediction_scores = [item for sublist in prediction_set for item in sublist]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tk4NmFh0KjLu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "f_score = f1_score(test_labels, prediction_scores, average='macro')\n",
        "precision = precision_score(test_labels, prediction_scores, average='macro')\n",
        "recall = recall_score(test_labels, prediction_scores, average='macro')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BQaJueaUKlQf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "8f75d257-58dc-42a9-9b40-72c68e14cae2"
      },
      "source": [
        "print(\"F-Score: \", f_score)\n",
        "print(\"Recall: \", recall)\n",
        "print(\"Precision: \", precision)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "F-Score:  0.925512644500616\n",
            "Recall:  0.9283536663275797\n",
            "Precision:  0.9238983955338487\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xTaAxtnyKnzf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "report = pd.DataFrame(classification_report(test_labels, prediction_scores, output_dict=True))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CyJ5RCD-Kqur",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "report = report.rename(columns={'0':'dunya',\n",
        "                          '1':'ekonomi',\n",
        "                          '2':'kultur',\n",
        "                          '3':'saglik',\n",
        "                          '4':'siyaset',\n",
        "                          '5':'spor',\n",
        "                          '6':'teknoloji'})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zjp7m2NRMoRp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "outputId": "c6c8e784-73f3-4cf4-c682-cbbf694e97c0"
      },
      "source": [
        "report"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>dunya</th>\n",
              "      <th>ekonomi</th>\n",
              "      <th>kultur</th>\n",
              "      <th>saglik</th>\n",
              "      <th>siyaset</th>\n",
              "      <th>spor</th>\n",
              "      <th>teknoloji</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>macro avg</th>\n",
              "      <th>weighted avg</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>precision</th>\n",
              "      <td>0.904412</td>\n",
              "      <td>0.918033</td>\n",
              "      <td>0.922222</td>\n",
              "      <td>0.921739</td>\n",
              "      <td>0.921875</td>\n",
              "      <td>0.982456</td>\n",
              "      <td>0.896552</td>\n",
              "      <td>0.923264</td>\n",
              "      <td>0.923898</td>\n",
              "      <td>0.923274</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>recall</th>\n",
              "      <td>0.924812</td>\n",
              "      <td>0.829630</td>\n",
              "      <td>0.954023</td>\n",
              "      <td>0.972477</td>\n",
              "      <td>0.880597</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.936937</td>\n",
              "      <td>0.923264</td>\n",
              "      <td>0.928354</td>\n",
              "      <td>0.923264</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>f1-score</th>\n",
              "      <td>0.914498</td>\n",
              "      <td>0.871595</td>\n",
              "      <td>0.937853</td>\n",
              "      <td>0.946429</td>\n",
              "      <td>0.900763</td>\n",
              "      <td>0.991150</td>\n",
              "      <td>0.916300</td>\n",
              "      <td>0.923264</td>\n",
              "      <td>0.925513</td>\n",
              "      <td>0.922616</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>support</th>\n",
              "      <td>133.000000</td>\n",
              "      <td>135.000000</td>\n",
              "      <td>87.000000</td>\n",
              "      <td>109.000000</td>\n",
              "      <td>134.000000</td>\n",
              "      <td>112.000000</td>\n",
              "      <td>111.000000</td>\n",
              "      <td>0.923264</td>\n",
              "      <td>821.000000</td>\n",
              "      <td>821.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                dunya     ekonomi  ...   macro avg  weighted avg\n",
              "precision    0.904412    0.918033  ...    0.923898      0.923274\n",
              "recall       0.924812    0.829630  ...    0.928354      0.923264\n",
              "f1-score     0.914498    0.871595  ...    0.925513      0.922616\n",
              "support    133.000000  135.000000  ...  821.000000    821.000000\n",
              "\n",
              "[4 rows x 10 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    }
  ]
}